{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"my_custom_rise.css\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "# Charge le CSS personnalis√©\n",
    "display(HTML('''<link rel=\"stylesheet\" type=\"text/css\" href=\"my_custom_rise.css\">'''))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 1: Introduction\n",
    "**A few useful things to know about machine learning**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Auto-setup when running on Google Colab\n",
    "import os\n",
    "if 'google.colab' in str(get_ipython()) and not os.path.exists('/content/master'):\n",
    "    !git clone -q https://github.com/ML-course/master.git /content/master\n",
    "    !pip --quiet install -r /content/master/requirements_colab.txt\n",
    "    %cd master/notebooks\n",
    "\n",
    "# Global imports and settings\n",
    "%matplotlib inline\n",
    "from preamble import *\n",
    "interactive = True # Set to True for interactive plots\n",
    "if interactive:\n",
    "    fig_scale = 1.5\n",
    "else: # For printing\n",
    "    fig_scale = 0.3\n",
    "    plt.rcParams.update(print_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Machine Learning?\n",
    "- Search engines (e.g. Google)\n",
    "- Recommender systems (e.g. Netflix)\n",
    "- Automatic translation (e.g. Google Translate)\n",
    "- Speech understanding (e.g. Siri, Alexa)\n",
    "- Game playing (e.g. AlphaGo)\n",
    "- Self-driving cars\n",
    "- Personalized medicine\n",
    "- Progress in all sciences: Genetics, astronomy, chemistry, neurology, physics,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Machine Learning?\n",
    "- Learn to perform a task, based on experience (examples) $X$, minimizing error $\\mathcal{E}$\n",
    "    - E.g. recognizing a person in an image as accurately as possible\n",
    "- Often, we want to learn a function (model) $f$ with some model parameters $\\theta$ that produces the right output $y$\n",
    "\n",
    "$$f_{\\theta}(X) = y$$\n",
    "$$\\underset{\\theta}{\\operatorname{argmin}} \\mathcal{E}(f_{\\theta}(X))$$\n",
    "\n",
    "- Usually part of a _much_ larger system that provides the data $X$ in the right form\n",
    "    - Data needs to be collected, cleaned, normalized, checked for data biases,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inductive bias\n",
    "\n",
    "- In practice, we have to put assumptions into the model: _inductive bias_ $b$\n",
    "    - What should the model look like?\n",
    "        - Mimick human brain: Neural Networks\n",
    "        - Logical combination of inputs: Decision trees, Linear models\n",
    "        - Remember similar examples: Nearest Neighbors, SVMs\n",
    "        - Probability distribution: Bayesian models\n",
    "    - User-defined settings (hyperparameters)\n",
    "        - E.g. depth of tree, network architecture\n",
    "    - Assuptions about the data distribution, e.g. $X \\sim N(\\mu,\\sigma)$\n",
    "- We can _transfer_ knowledge from previous tasks: $f_1, f_2, f_3, ... \\Longrightarrow f_{new}$\n",
    "    - Choose the right model, hyperparameters\n",
    "    - Reuse previously learned values for model parameters $\\theta$\n",
    "- In short:\n",
    "\n",
    "$$\\underset{\\theta,b}{\\operatorname{argmin}} \\mathcal{E}(f_{\\theta, b}(X))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine learning vs Statistics\n",
    "* See Breiman (2001): Statistical modelling: The two cultures\n",
    "* Both aim to make predictions of natural phenomena:\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/00_stat1.png\" alt=\"ml\" style=\"margin-left:10px; width:200px\"/>\n",
    "\n",
    "* Statistics:\n",
    "    - Help humans understand the world\n",
    "    - Assume data is generated according to an understandable model\n",
    "    <img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/00_stat2.png\" alt=\"ml\" style=\"margin-left:10px; width:200px\"/>\n",
    "\n",
    "* Machine learning:\n",
    "    - Automate a task entirely (partially _replace_ the human)\n",
    "    - Assume that the data generation process is unknown\n",
    "    - Engineering-oriented, less (too little?) mathematical theory\n",
    "    <img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/00_stat3.png\" alt=\"ml\" style=\"margin-left:10px; width:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of machine learning\n",
    "- __Supervised Learning__: learn a _model_ $f$ from _labeled data_ $(X,y)$ (ground truth)\n",
    "    - Given a new input _X_, predict the right output _y_\n",
    "    - Given examples of stars and galaxies, identify new objects in the sky\n",
    "- __Unsupervised Learning__: explore the structure of the data (X) to extract meaningful information\n",
    "    - Given inputs _X_, find which ones are special, similar, anomalous, ...\n",
    "- __Semi-Supervised Learning__: learn a model from (few) labeled and (many) unlabeled examples\n",
    "    - Unlabeled examples add information about which new examples are likely to occur\n",
    "- __Reinforcement Learning__: develop an agent that improves its performance based on interactions with the environment \n",
    "\n",
    "Note: Practical ML systems can combine many types in one system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised Machine Learning\n",
    "\n",
    "- Learn a model from labeled training data, then make predictions\n",
    "- Supervised: we know the correct/desired outcome (label)\n",
    "- Subtypes: _classification_ (predict a class) and _regression_ (predict a numeric value)\n",
    "- Most supervised algorithms that we will see can do both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/01_supervised.png\" alt=\"ml\" style=\"width:60%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Classification\n",
    "\n",
    "- Predict a _class label_ (category), discrete and unordered\n",
    "    - Can be _binary_ (e.g. spam/not spam) or _multi-class_ (e.g. letter recognition)\n",
    "    - Many classifiers can return a _confidence_ per class\n",
    "- The predictions of the model yield a _decision boundary_ separating the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "# create a synthetic dataset\n",
    "X1, y1 = make_moons(n_samples=70, noise=0.2, random_state=8)\n",
    "\n",
    "# Train classifiers\n",
    "lr = LogisticRegression().fit(X1, y1)\n",
    "svm = SVC(kernel='rbf', gamma=2, probability=True).fit(X1, y1)\n",
    "knn = KNeighborsClassifier(n_neighbors=3).fit(X1, y1)\n",
    "\n",
    "# Plotting\n",
    "@interact\n",
    "def plot_classifier(classifier=[lr,svm,knn]):  \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12*fig_scale, 4*fig_scale))\n",
    "    mglearn.tools.plot_2d_separator(\n",
    "        classifier, X1, ax=axes[0], alpha=.4, cm=mglearn.cm2)\n",
    "    scores_image = mglearn.tools.plot_2d_scores(\n",
    "        classifier, X1, ax=axes[1], alpha=.5, cm=mglearn.ReBl, function='predict_proba')\n",
    "    for ax in axes:\n",
    "        mglearn.discrete_scatter(X1[:, 0], X1[:, 1], y1,\n",
    "                                 markers='.', ax=ax)\n",
    "        ax.set_xlabel(\"Feature 0\")\n",
    "        ax.set_ylabel(\"Feature 1\", labelpad=0)\n",
    "        ax.tick_params(axis='y', pad=0)\n",
    "\n",
    "    cbar = plt.colorbar(scores_image, ax=axes.tolist())\n",
    "    cbar.set_label('Predicted probability', rotation=270, labelpad=6)\n",
    "    cbar.set_alpha(1)\n",
    "    axes[0].legend([\"Class 0\", \"Class 1\"], ncol=4, loc=(.1, 1.1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "# create a synthetic dataset\n",
    "X1, y1 = make_moons(n_samples=70, noise=0.2, random_state=8)\n",
    "\n",
    "# Train classifiers\n",
    "lr = LogisticRegression().fit(X1, y1)\n",
    "svm = SVC(kernel='rbf', gamma=2, probability=True).fit(X1, y1)\n",
    "knn = KNeighborsClassifier(n_neighbors=3).fit(X1, y1)\n",
    "\n",
    "# Create a dictionary of classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": lr,\n",
    "    \"SVM (RBF Kernel)\": svm,\n",
    "    \"KNN (k=3)\": knn\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "@interact(classifier=list(classifiers.keys()))\n",
    "def plot_classifier(classifier):\n",
    "    selected_model = classifiers[classifier]  # Get the selected classifier\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    mglearn.tools.plot_2d_separator(\n",
    "        selected_model, X1, ax=axes[0], alpha=.4, cm=mglearn.cm2)\n",
    "    scores_image = mglearn.tools.plot_2d_scores(\n",
    "        selected_model, X1, ax=axes[1], alpha=.5, cm=mglearn.ReBl, function='predict_proba')\n",
    "    for ax in axes:\n",
    "        mglearn.discrete_scatter(X1[:, 0], X1[:, 1], y1,\n",
    "                                 markers='.', ax=ax)\n",
    "        ax.set_xlabel(\"Feature 0\")\n",
    "        ax.set_ylabel(\"Feature 1\", labelpad=0)\n",
    "        ax.tick_params(axis='y', pad=0)\n",
    "\n",
    "    cbar = plt.colorbar(scores_image, ax=axes.tolist())\n",
    "    cbar.set_label('Predicted probability', rotation=270, labelpad=6)\n",
    "    cbar.set_alpha(1)\n",
    "    axes[0].legend([\"Class 0\", \"Class 1\"], ncol=4, loc=(.1, 1.1));\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    plot_classifier(classifier=svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Example: Flower classification\n",
    "Classify types of Iris flowers (setosa, versicolor, or virginica). How would you do it?\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/01_iris.jpeg\" alt=\"ml\" style=\"width: 75%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Representation: input features and labels\n",
    "- We could take pictures and use them (pixel values) as inputs (-> Deep Learning)\n",
    "- We can manually define a number of input features (variables), e.g. length and width of leaves\n",
    "- Every `example' is a point in a (possibly high-dimensional) space\n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/01_terminology.png\" alt=\"ml\" style=\"float: left; width: 50%;\"/>\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/01_iris3d.png\" alt=\"ml\" style=\"float: left; width: 35%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Regression\n",
    "- Predict a continuous value, e.g. temperature\n",
    "    - Target variable is numeric\n",
    "    - Some algorithms can return a _confidence interval_\n",
    "- Find the relationship between predictors and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from mglearn.datasets import make_wave\n",
    "from mglearn.plot_helpers import cm2\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from preamble import *\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "X2, y2 = make_wave(n_samples=60)\n",
    "x = np.atleast_2d(np.linspace(-3, 3, 100)).T\n",
    "lr = LinearRegression().fit(X2, y2)\n",
    "ridge = BayesianRidge().fit(X2, y2)\n",
    "gp = GaussianProcessRegressor(kernel=RBF(10, (1e-2, 1e2)), n_restarts_optimizer=9, alpha=0.1, normalize_y=True).fit(X2, y2)\n",
    "\n",
    "@interact\n",
    "def plot_regression(regressor=[lr, ridge, gp]):\n",
    "    line = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "    plt.figure(figsize=(5*fig_scale, 5*fig_scale))\n",
    "    plt.plot(X2, y2, 'o', c=cm2(0))\n",
    "    if(regressor.__class__.__name__ == 'LinearRegression'):\n",
    "        y_pred = regressor.predict(x)\n",
    "    else:\n",
    "        y_pred, sigma = regressor.predict(x, return_std=True)\n",
    "        plt.fill(np.concatenate([x, x[::-1]]),\n",
    "             np.concatenate([y_pred - 1.9600 * sigma,\n",
    "                            (y_pred + 1.9600 * sigma)[::-1]]),\n",
    "             alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "        \n",
    "    plt.plot(line, y_pred, 'b-')\n",
    "    plt.xlabel(\"Input feature 1\")\n",
    "    plt.ylabel(\"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    plot_regression(regressor=gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unsupervised Machine Learning\n",
    "\n",
    "- Unlabeled data, or data with unknown structure\n",
    "- Explore the structure of the data to extract information\n",
    "- Many types, we'll just discuss two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Clustering\n",
    "\n",
    "- Organize information into meaningful subgroups (clusters)\n",
    "- Objects in cluster share certain degree of similarity (and dissimilarity to other clusters)\n",
    "- Example: distinguish different types of customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Note: the most recent versions of numpy seem to cause problems for KMeans\n",
    "# Uninstalling and installing the latest version of threadpoolctl fixes this\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "nr_samples = 1500\n",
    "\n",
    "@interact\n",
    "def plot_clusters(randomize=(1,100,1)):\n",
    "    # Generate data\n",
    "    X, y = make_blobs(n_samples=nr_samples, cluster_std=[1.0, 1.5, 0.5], random_state=randomize)\n",
    "    # Cluster\n",
    "    y_pred = KMeans(n_clusters=3, random_state=randomize).fit_predict(X)\n",
    "    # PLot\n",
    "    plt.figure(figsize=(5*fig_scale, 5*fig_scale))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "    plt.title(\"KMeans Clusters\")\n",
    "    plt.xlabel(\"Feature 0\")\n",
    "    plt.ylabel(\"Feature 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    plot_clusters(randomize=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Dimensionality reduction\n",
    "\n",
    "- Data can be very high-dimensional and difficult to understand, learn from, store,...\n",
    "- Dimensionality reduction can compress the data into fewer dimensions, while retaining most of the information\n",
    "- Contrary to feature selection, the new features lose their (original) meaning\n",
    "- The new representation can be a lot easier to model (and visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.manifold import locally_linear_embedding\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "X, color = make_swiss_roll(n_samples=800, random_state=123)\n",
    "\n",
    "fig = plt.figure(figsize=plt.figaspect(0.3)*fig_scale*2.5)\n",
    "ax1 = fig.add_subplot(1, 3, 1, projection='3d')\n",
    "ax1.xaxis.pane.fill = False\n",
    "ax1.yaxis.pane.fill = False\n",
    "ax1.zaxis.pane.fill = False\n",
    "ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.rainbow, s=10*fig_scale)\n",
    "plt.title('Swiss Roll in 3D')\n",
    "\n",
    "ax2 = fig.add_subplot(1, 3, 2)\n",
    "scikit_pca = PCA(n_components=2)\n",
    "X_spca = scikit_pca.fit_transform(X)\n",
    "plt.scatter(X_spca[:, 0], X_spca[:, 1], c=color, cmap=plt.cm.rainbow)\n",
    "plt.title('PCA');\n",
    "\n",
    "ax3 = fig.add_subplot(1, 3, 3)\n",
    "X_lle, err = locally_linear_embedding(X, n_neighbors=12, n_components=2)\n",
    "plt.scatter(X_lle[:, 0], X_lle[:, 1], c=color, cmap=plt.cm.rainbow)\n",
    "plt.title('Locally Linear Embedding');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reinforcement learning\n",
    "\n",
    "- Develop an agent that improves its performance based on interactions with the environment\n",
    "    - Example: games like Chess, Go,...\n",
    "- Search a (large) space of actions and states\n",
    "- _Reward function_ defines how well a (series of) actions works\n",
    "- Learn a series of actions (policy) that maximizes reward through exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/01_rl2.png\" alt=\"ml\" style=\"width: 50%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning = Representation + evaluation + optimization\n",
    "All machine learning algorithms consist of 3 components:\n",
    "- **Representation**: A model $f_{\\theta}$ must be represented in a formal language that the computer can handle\n",
    "    - Defines the 'concepts' it can learn, the _hypothesis space_\n",
    "    - E.g. a decision tree, neural network, set of annotated data points\n",
    "- **Evaluation**: An _internal_ way to choose one hypothesis over the other\n",
    "    - Objective function, scoring function, loss function $\\mathcal{L}(f_{\\theta})$\n",
    "    - E.g. Difference between correct output and predictions\n",
    "- **Optimization**: An _efficient_ way to search the hypothesis space\n",
    "    - Start from simple hypothesis, extend (relax) if it doesn't fit the data\n",
    "    - Start with initial set of model parameters, gradually refine them\n",
    "    - Many methods, differing in speed of learning, number of optima,...\n",
    "    \n",
    "A powerful/flexible model is only useful if it can also be optimized efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural networks: representation\n",
    "Let's take neural networks as an example\n",
    "* Representation: (layered) neural network\n",
    "    * Each connection has a _weight_ $\\theta_i$ (a.k.a. model parameters)\n",
    "    * Each node receives weighted inputs, emits new value\n",
    "    * Model $f$ returns the output of the last layer    \n",
    "* The architecture, number/type of neurons, etc. are fixed\n",
    "    * We call these _hyperparameters_ (set by user, fixed during training)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/08_nn_basic_arch.png\" alt=\"ml\" style=\"width: 40%;\"/>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural networks: evaluation and optimization\n",
    "* Representation: Given the structure, the model is represented by its parameters\n",
    "    * Imagine a mini-net with two weights ($\\theta_0,\\theta_1$): a 2-dimensional search space\n",
    "* Evaluation: A _loss function_ $\\mathcal{L}(\\theta)$ computes how good the predictions are\n",
    "    * _Estimated_ on a set of training data with the 'correct' predictions\n",
    "    * We can't see the full surface, only evaluate specific sets of parameters\n",
    "* Optimization: Find the optimal set of parameters\n",
    "    * Usually a type of _search_ in the hypothesis space\n",
    "    * E.g. Gradient descent: $\\theta_i^{new} = \\theta_i - \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta_i} $\n",
    "\n",
    "    \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/00_ml3.png\" alt=\"ml\" style=\"float: left; width: 90%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overfitting and Underfitting\n",
    "* It's easy to build a complex model that is 100% accurate on the training data, but very bad on new data\n",
    "* Overfitting: building a model that is _too complex for the amount of data_ you have\n",
    "    * You model peculiarities in your training data (noise, biases,...)\n",
    "    * Solve by making model simpler (regularization), or getting more data\n",
    "    * **Most algorithms have hyperparameters that allow regularization**\n",
    "* Underfitting: building a model that is _too simple given the complexity of the data_\n",
    "    * Use a more complex model\n",
    "* There are techniques for detecting overfitting (e.g. bias-variance analysis). More about that later\n",
    "* You can build _ensembles_ of many models to overcome both underfitting and overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* There is often a sweet spot that you need to find by optimizing the choice of algorithms and hyperparameters, or using more data.  \n",
    "* Example: regression using polynomial functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "np.random.seed(0)\n",
    "n_samples = 30\n",
    "X3 = np.sort(np.random.rand(n_samples))\n",
    "y3 = true_fun(X3) + np.random.randn(n_samples) * 0.1\n",
    "X3_test = np.linspace(0, 1, 100)\n",
    "scores_x, scores_y = [], []\n",
    "\n",
    "show_output = True\n",
    "\n",
    "@interact\n",
    "def plot_poly(degrees = (1, 16, 1)):\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees,\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X3[:, np.newaxis], y3)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X3[:, np.newaxis], y3,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)   \n",
    "    scores_x.append(degrees)\n",
    "    scores_y.append(-scores.mean())\n",
    "\n",
    "    if show_output:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12*fig_scale, 4*fig_scale))    \n",
    "        ax1.plot(X3_test, pipeline.predict(X3_test[:, np.newaxis]), label=\"Model\")\n",
    "        ax1.plot(X3_test, true_fun(X3_test), label=\"True function\")\n",
    "        ax1.scatter(X3, y3, edgecolor='b', label=\"Samples\")\n",
    "        ax1.set_xlabel(\"x\")\n",
    "        ax1.set_ylabel(\"y\")\n",
    "        ax1.set_xlim((0, 1))\n",
    "        ax1.set_ylim((-2, 2))\n",
    "        ax1.legend(loc=\"best\")\n",
    "        ax1.set_title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "            degrees, -scores.mean(), scores.std()))\n",
    "\n",
    "        # Plot scores\n",
    "        ax2.scatter(scores_x, scores_y, edgecolor='b')\n",
    "        order = np.argsort(scores_x)\n",
    "        ax2.plot(np.array(scores_x)[order], np.array(scores_y)[order])\n",
    "        ax2.set_xlim((0, 16))\n",
    "        ax2.set_ylim((10**-2, 10**11))\n",
    "        ax2.set_xlabel(\"degree\")\n",
    "        ax2.set_ylabel(\"error\", labelpad=0)\n",
    "        ax2.set_yscale(\"log\")\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from ipywidgets import IntSlider, Output\n",
    "\n",
    "if not interactive:\n",
    "    show_output = False\n",
    "    for i in range(1,15):\n",
    "        plot_poly(degrees = i)\n",
    "    \n",
    "    show_output = True\n",
    "    plot_poly(degrees = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model selection\n",
    "- Next to the (internal) loss function, we need an (external) evaluation function\n",
    "    - Feedback signal: are we actually learning the right thing? \n",
    "        - Are we under/overfitting?\n",
    "    - Carefully choose to fit the application.\n",
    "    - Needed to select between models (and hyperparameter settings)\n",
    "    \n",
    "&copy; XKCD\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/xkcd.jpg\" alt=\"ml\" style=\"width: 30%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Data needs to be split into _training_ and _test_ sets\n",
    "    - Optimize model parameters on the training set, evaluate on independent test set\n",
    "- Avoid _data leakage_:\n",
    "    - Never optimize hyperparameter settings on the test data\n",
    "    - Never choose preprocessing techniques based on the test data\n",
    "- To optimize hyperparameters and preprocessing as well, set aside part of training set as a _validation_ set\n",
    "    - Keep test set hidden during _all_ training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import mglearn\n",
    "mglearn.plots.plot_threefold_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* For a given hyperparameter setting, learn the model parameters on training set\n",
    "    * Minize the loss\n",
    "* Evaluate the trained model on the validation set\n",
    "    * Tune the hyperparameters to maximize a certain metric (e.g. accuracy)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/00_hyperparams.png\" alt=\"ml\" style=\"width: 40%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Only generalization counts!\n",
    "* Never evaluate your final models on the training data, except for:\n",
    "    * Tracking whether the optimizer converges (learning curves)\n",
    "    * Diagnosing under/overfitting:\n",
    "        * Low training and test score: underfitting\n",
    "        * High training score, low test score: overfitting\n",
    "* Always keep a completely independent test set \n",
    "* On small datasets, use multiple train-test splits to avoid sampling bias\n",
    "    * You could sample an 'easy' test set by accident\n",
    "    * E.g. Use cross-validation (see later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Better data representations, better models\n",
    "- Algorithm needs to correctly transform the inputs to the right outputs\n",
    "- A lot depends on how we present the data to the algorithm\n",
    "    - Transform data to better representation (a.k.a. _encoding_ or _embedding_)\n",
    "    - Can be done end-to-end (e.g. deep learning) or by first 'preprocessing' the data (e.g. feature selection/generation)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/00_representation.png\" alt=\"ml\" style=\"width: 80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature engineering\n",
    "* Most machine learning techniques require humans to build a good representation of the data  \n",
    "* Especially when data is naturally structured (e.g. table with meaningful columns)\n",
    "* Feature engineering is often still necessary to get the best results\n",
    "    * Feature selection, dimensionality reduction, scaling, ...\n",
    "    * *Applied machine learning is basically feature engineering (Andrew Ng)*\n",
    "* Nothing beats domain knowledge (when available) to get a good representation\n",
    "    * E.g. Iris data: leaf length/width separate the classes well\n",
    "    \n",
    "Build prototypes early-on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning data transformations end-to-end\n",
    "* For unstructured data (e.g. images, text), it's hard to extract good features\n",
    "* Deep learning: learn your own representation (embedding) of the data \n",
    "    * Through multiple layers of representation (e.g. layers of neurons)\n",
    "    * Each layer transforms the data a bit, based on what reduces the error\n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/00_layers.png\" alt=\"ml\" style=\"width: 60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example: digit classification\n",
    "- Input pixels go in, each layer transforms them to an increasingly informative representation for the given task\n",
    "- Often less intuitive for humans\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/00_layers2.png\" alt=\"ml\" style=\"width: 60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Curse of dimensionality\n",
    "* Just adding lots of features and letting the model figure it out doesn't work\n",
    "* Our assumptions (inductive biases) often fail in high dimensions: \n",
    "    - Randomly sample points in an n-dimensional space (e.g. a unit hypercube)\n",
    "    - Almost all points become outliers at the edge of the space\n",
    "    - Distances between any two points will become almost identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Code originally by Peter Norvig \n",
    "def sample(d=2, N=100):\n",
    "    return [[np.random.uniform(0., 1.) for i in range(d)] for _ in range(N)]\n",
    "\n",
    "def corner_count(points):\n",
    "    return np.mean([any([(d < .01 or d > .99) for d in p]) for p in points])\n",
    "\n",
    "def go(Ds=range(1,200)):\n",
    "    plt.figure(figsize=(5*fig_scale, 4*fig_scale))\n",
    "    plt.plot(Ds, [corner_count(sample(d)) for d in Ds])\n",
    "    plt.xlabel(\"Number of dimensions\")\n",
    "    plt.ylabel(\"Proportion of point that are 1% outliers\")\n",
    "    \n",
    "go()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Practical consequences\n",
    "* For every dimension (feature) you add, you need exponentially more data to avoid sparseness\n",
    "* Affects any algorithm that is based on distances (e.g. kNN, SVM, kernel-based methods, tree-based methods,...)\n",
    "* Blessing of non-uniformity: on many applications, the data lives in a very small subspace\n",
    "    * You can drastically improve performance by selecting features or using lower-dimensional data representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"More data can beat a cleverer algorithm\"\n",
    "(but you need both)\n",
    "\n",
    "* More data reduces the chance of overfitting\n",
    "* Less sparse data reduces the curse of dimensionality\n",
    "* _Non-parametric_ models: number of model parameters grows with amount of data\n",
    "    - Tree-based techniques, k-Nearest neighbors, SVM,...\n",
    "    - They can learn any model given sufficient data (but can get stuck in local minima)\n",
    "* _Parametric_ (fixed size) models: fixed number of model parameters\n",
    "    - Linear models, Neural networks,...\n",
    "    - Can be given a huge number of parameters to benefit from more data \n",
    "    - Deep learning models can have millions of weights, learn almost any function.\n",
    "* The bottleneck is moving from data to compute/scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building machine learning systems\n",
    "A typical machine learning system has multiple components, which we will cover in upcoming lectures:\n",
    "    \n",
    "- Preprocessing: Raw data is rarely ideal for learning\n",
    "    - Feature scaling: bring values in same range\n",
    "    - Encoding: make categorical features numeric\n",
    "    - Discretization: make numeric features categorical\n",
    "    - Label imbalance correction (e.g. downsampling)\n",
    "    - Feature selection: remove uninteresting/correlated features\n",
    "    - Dimensionality reduction can also make data easier to learn\n",
    "    - Using pre-learned embeddings (e.g. word-to-vector, image-to-vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Learning and evaluation\n",
    "    - Every algorithm has its own biases\n",
    "    - No single algorithm is always best\n",
    "    - _Model selection_ compares and selects the best models\n",
    "        - Different algorithms, different hyperparameter settings\n",
    "    - Split data in training, validation, and test sets\n",
    "    \n",
    "- Prediction\n",
    "    - Final optimized model can be used for prediction\n",
    "    - Expected performance is performance measured on _independent_ test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Together they form a _workflow_ of _pipeline_\n",
    "- There exist machine learning methods to automatically build and tune these pipelines\n",
    "- You need to optimize pipelines continuously\n",
    "    - _Concept drift_: the phenomenon you are modelling can change over time\n",
    "    - _Feedback_: your model's predictions may change future data \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/01_pipeline2.png\" alt=\"ml\" style=\"width: 80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "* Learning algorithms contain 3 components:\n",
    "    - Representation: a model $f$ that maps input data $X$ to desired output $y$\n",
    "        - Contains model parameters $\\theta$ that can be made to fit the data $X$\n",
    "    - Loss function $\\mathcal{L}(f_{\\theta}(X))$: measures how well the model fits the data\n",
    "    - Optimization technique to find the optimal $\\theta$: $\\underset{\\theta}{\\operatorname{argmin}} \\mathcal{L}(f_{\\theta}(X))$\n",
    "* Select the right model, then fit it to the data to minimize a task-specific error $\\mathcal{E}$ \n",
    "    - Inductive bias $b$: assumptions about model and hyperparameters  \n",
    "    $\\underset{\\theta,b}{\\operatorname{argmin}} \\mathcal{E}(f_{\\theta, b}(X))$\n",
    "\n",
    "* Overfitting: model fits the training data well but not new (test) data\n",
    "    - Split the data into (multiple) train-validation-test splits\n",
    "    - Regularization: tune hyperparameters (on validation set) to simplify model\n",
    "    - Gather more data, or build ensembles of models\n",
    "* Machine learning _pipelines_: preprocessing + learning + deployment"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "latex_metadata": {
   "author": "Joaquin Vanschoren",
   "title": "Introduction"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
